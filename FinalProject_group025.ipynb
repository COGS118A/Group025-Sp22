{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steamy Reviews\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Merel van den Bos\n",
    "- Alex Rivera\n",
    "- Albert Aung\n",
    "- Lillian Wood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This project will focus on the online gaming storefront Steam and its review system. Our goal with the dataset is to create a model using sentiment analysis that can automatically detect whether a user's review of a game is positive or negative based on language alone. We will be using heavy feature selection, such as bag of words and weighing certain features. Then, we plan to run a classification model such as Naïve Bayes or a Decision Tree in an attempt to categorize incoming reviews into positive or negative. This will allow the review system to be more automatic and better show what the general consensus is about a particular game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Steam is a highly-used online marketplace for P.C. videogames. According to Statista, in 2020, approximately 120 million people were active monthly on Steam, demonstrating its wide reach <a name=\"Statista\"></a>[<sup>[4]</sup>](#Statista). The popularity of the platform has developed into a social feature by housing the P.C. gaming community and connecting friends through mutually-played games. An important feature of Steam, both as a marketplace and a social sphere, is the ability to write, read, and rate reviews for games.\n",
    "\n",
    "The plethora of reviews on Steam provides both an interesting and abundant source of text with potential to drive useful sentiment analysis models. In their paper, \"Steam Review Dataset - new, large scale sentiment dataset,\" Sobkowicz and Stokowiec introduce a dataset which they claim could be a powerful source of consumer data for sentiment analysis <a name=\"Sobkowicz\"></a>[<sup>[3]</sup>](#sobkowicznote). Utilizing these reviews has relevant and important implications. Since these reviews act as consumer data, conclusions drawn from these reviews inform both gamers and game developers about successful and worthwhile games. Additionally, the function of reviewing is a very powerful feature, providing gamers with a voice that directly impacts which games sell and which games flop <a name=\"Sobkowicz\"></a>[<sup>[3]</sup>](#sobkowicznote). It would be highly valuable for both development of future games and for satisfaction of gamers to tailor the review system to be effective and informative.\n",
    "\n",
    "As it stands, reviews consist of both written commentary and a \"Yes\" (thumbs-up) or \"No\" (thumbs-down). The Yes and No ratings for each game are averaged to create two summative ratings which appear underneath the synopsis of the game when a user views the game's page. \"Recent Reviews\" averages the number of positive and negative recent reviews. \"All\" averages the number of all positive and negative reviews. The summative ratings are labeled on a scale of: overwhelmingly negative, very negative, negative, mostly negative, mixed, mostly positive, positive, very positive, overwhelmingly positive.\n",
    "\n",
    "Although this feedback is already incredibly helpful to gamers and game developers, it is questionable whether the data is entirely accurate. As they stand, the summative ratings are binary, only based on \"Yes\" or \"No.\" There is no way to account for partially liking or disliking a game. Forcing users to choose between Yes or No may skew the ratings incorrectly, since there is no middle ground. Therefore, it would be useful to also summarily analyze the written data in order to develop a more well-rounded summary of a game's reviews.\n",
    "\n",
    "Previous research into sentiment analysis of Steam reviews shows that both Naïve Bayes and Decision Tree classification are useful models in predicting review sentiment <a name=\"zuo\"></a>[<sup>[5]</sup>](#zuonote). It has been found that decision tree classification is a better-performing model than Naïve Bayes <a name=\"zuo\"></a>[<sup>[5]</sup>](#zuonote). We aim to test this, by ultimately attempting both classification methods to predict positive and negative review sentiment. Based on our results, either confirming or contradicting prior research, we will select the best model to use for this sentiment analysis.\n",
    "\n",
    "Additionally, a major aspect of this project will be feature selection of the textual Steam reviews in order to perform classification.  Feature selection is essential to developing more accurate classification models <a name=\"formannote\"></a>[<sup>[1]</sup>](#forman), therefore we find it necessary to perform feature selection on our textual data. Multiple feature selection methods exist for sentiment analysis, such as ngrams, bag of words, information gain (IG) and sentiment lexicons <a name=\"Sharma\"></a>[<sup>[2]</sup>](#sharmanote). However, this research targets other platforms, such as movie reviews which does not account for the language that is unique to gamers. Moreover, prior research accounts for removing special characters/digits, lower case, removing stop words, stemming, removing links, removing most frequent or most infrequent words, misspelled words, and short reviews <a name=\"Zuo\"></a>[<sup>[5]</sup>](#zuonote). Yet, limitations include a lack of testing to see the effectiveness of different feature selections. Another concern is that these methods that are successful in other linguistic spheres but may not account for niche gamer-speak, sarcasm, emojis, and emoticons. This could all be very useful and valuable information, which we do not want to lose when trying to classify positive and negative reviews.  Further research is needed in regards to effective feature selection for sentiment analysis of Steam gaming reviews. Therefore, we propose our own solution. We will begin by using bag of words as our feature selection method. We plan to account for unique gamer speak by placing weights on certain characters, emojis, and emoticons. \n",
    "\n",
    "We propose the most meaningful way to analyze and summarize a game's written reviews would be to through heavy feature selection, weighing special characters and unique gamer speech, and utlizing decision tree or Naïve Bayes classification to predict the rating of a game based on the written reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "\n",
    "Problem: Optimizing the prediction of game ratings based on user reviews.\n",
    "\n",
    "Solution: Sentiment Analysis through bag of words feature selection and weighing special characters and gamer speech, followed by Naïve Bayes or decision tree Classification.\n",
    "\n",
    "Metrics of Measurement: Accuracy (percentage of correct game rating predictions) Precision (percentage of correct game rating predictions over correct game rating predictions and false correct game rating predictions) Recall (percentage of correct game rating predictions over correct game rating predicitions and false incorrect game rating predictions).\n",
    "\n",
    "Replicability: Choosing a large dataset of 6.4 million observations made and making it available for everyone to access Looking to avoid overfitting Using easily-accessible libraries for creating different models which are shared online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "- Dataset: https://www.kaggle.com/datasets/andrewmvd/steam-reviews\n",
    "- Description: The dataset contains over 6.4 million observations, which are publicly available reviews in English from the Steam Reviews portion of Steam store run by Valve. 5 variables describe each observation: Game id, Game Name, Review text, Review Sentiment: whether the game the review recommends the game or not, and Review vote: whether the review was recommended by another user or not.\n",
    "- Some critical variables are the Review text, Review sentiment, and Review vote. Review text will be string data. Review sentiment is coded -1 as negative and 1 as positive review. Review vote is coded 0 as not recommended and 1 as recommended.\n",
    "- Review sentiment and Review vote are already in numerical values, which alleviates cleaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "The solution to the problem we're trying to solve is a classification, such as Naïve Bayes or Decision Trees, to perform sentiment analysis. Sentiment analysis classification is an ideal model because our problem involves evaluating user ratings on Steam. Sentiment analysis looks into analyzing text to classify it. In our case, classification is between binary options (i.e. whether a review is positive or negative). Moreover, gaining this information allows us to make predictions on the ratings of each game. \n",
    "\n",
    "To do this, we will apply different models to see which model optimizes the correct prediction of game ratings. To perform sentiment analysis, we will pre-process data to reduce noise and account for dimensionality to improve the efficiency of the machine learning models. Some ways we look to do this include cleaning the data by switching all the words into lowercase, removing numbers, removing stopwords and removing punctuation <a name=\"zuo\"></a>[<sup>[5]</sup>](#zuonote). As previously mentioned, it is of necessity to perform at least one feature selection method. Because prior work in this area is lacking, we propose our own solution. We will begin by using bag of words as our feature selection method. Unique gamer speak will be accounted for by placing weights on certain characters, emojis, and emoticons. \n",
    "\n",
    "After performing feature selection, we aim to use a classification method  for sentiment analysis such as Naïve Bayes or Decision Trees. There is evidence to support the use of both models, although Decision Trees was shown to be more accurate <a name=\"zuo\"></a>[<sup>[5]</sup>](#zuonote). However, we believe it will be useful to try both models in an attempt to identify the best-performing classification model, especially using our unique feature selection. Using one of these classification models will then allow us to automatically and more accurately categorize reviews into positive and negative without further user input.\n",
    "\n",
    "Essentially, we propose the most meaningful way to analyze and summarize a game's written reviews would be to through bag of words feature selection, weighing special characters and unique gamer speech, and utilizing decision tree or Naïve Bayes classification to predict the rating of a game based on the written reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Since the problem we are tackling is a classifcation problem (i.e. whether a rating is positive or negative), our evaluation metrics in relation to sentiment analysis will include the following: precision, recall, f-score and accuracy. Accuracy or more specifically classification accuracy can be determined by the formula (Accuracy = Number of Correct Predictions / Total number of predictions made). This measures the correctness of predictions as suggested by the formula. An equation that envelopes both precision and recall is the calculation of the F1 score which entails (F1 = 2 * 1/(1/Precision + 1/Recall)). The F1 Score tells us how precise (preicison) and how error-less our model is (recall). A high amount of precision and low amount of recall can lead to a significant number of missing instances and a low amount of precision but high amount of recall shows us inaccurate the data is but it does not miss a significant number of instances. The F1 score which ranges from [0,1] calculates and tries to tell us the balance between precision and recall. The prediction formula is given by (Precision = Number of True Positives / ( Number of True Positives + Number of False Positives)) and tells us the number of correct positive results over the number of positive results predicted by the model. The recall formula is given by ( Number of True Positives / Number of True Positives + Number of False Negatives) and tells us the number of correct positive results over the number of all samples that should have identified as positive. \n",
    "\n",
    "Source: https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Our model performed relatively well compared with previous sentiment analysis models of Steam reviews <a name=\"zuo\"></a>[<sup>[5]</sup>](#zuonote). Due to the large size of our dataset, we are not concerned with having too little data points to analyze. \n",
    "\n",
    "However, our model has its limitations. We initially planned to weigh certain characters and emoticons during feature selection to see if it would improve our model. However, within the scope of this project, we were unable to explore the impact that weighing emoticons and unique gamer speech would have on our model's precision, accuracy, and recall. Instead, we opted to track the top most important features to each classification model. We feel that this provides useful insight to improve feature selection for similar models in the future. It would be interesting to explore whether adding weights to these features, or other unique features, would improve the outcomes of different classification models.\n",
    "\n",
    "Furthermore, our model should only be used to analyze sentiment in Steam reviews. Gamer speech is niche compared to other platforms or review sites, and our model is specifically selected towards this speech.\n",
    "\n",
    "Lastly, by nature the frequency of certain reviews over others may limit the effectiveness of our model. Positive reviews are more prevalent in our data than negative reviews. Our data has less training instances to correctly analyze negative sentiment. Also, certain games have much more reviews  over others. Analyzing sentiment towards games which are less widespread, such as niche indie games, may be less effective than our model is at analyzing sentiment towards popular games. \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "For our ethical review, we consulted the ‘’Data Science Ethics Checklist’’ by deon (https://deon.drivendata.org/), which contains a checklist of items for every data-related project. When people make a review on Steam, they get the option to choose whether it is public, only for their friends, or private. Our data is accessed from Kaggle which offers data to everyone who credits the party that has posted their dataset which we have done so. Nevertheless, we have also chosen a dataset with largely anonymous data for privacy reasons and avoid creating a potential bias by including personal information like gender for instance. \n",
    "\n",
    "Our data set does not include the users rather only user_ids which have been generated and have been kept confidential and the only identifying information is the Steam reviews themselves. Besides that, we also avoid the risk of unintended harm by choosing the anonymous data, where we avoid online abuse or physical harm being conducted upon the participants since their private information will be kept private in every way. Also, since some reviews are already private, this equalizes all the reviews together and Steam has provided this data. Their own website does have a privacy policy about for their users (https://store.steampowered.com/privacy_agreement/) so people must approve that. Despite the anonymity, we will also not use the data for any other purposes than our project. We have chosen a huge dataset which hopefully minimizes our potential bias. \n",
    "\n",
    "We are very aware of potential risks like p-hacking and wrongly sampling sizing and will pay attention to those. Any very odd outliers will be investigated and taken seriously. We know the source of our data and will share the dataset in our proposal for total transparency.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"formannote\"></a>1.[^](#forman): Forman, G. (2003). An extensive empirical study of feature selection metrics for text classification. The Journal of Machine Learning Research, 3, pp. 1289-1305.<br>\n",
    "<a name=\"sharmanote\"></a>2.[^](#sharma): Sharma & Dey. (2012). Performance Investigation of Feature Selection Methods and Sentiment Lexicons for Sentiment Analysis. Special Issue of International Journal of Computer Applications, June 2012.<br>\n",
    "<a name=\"sobkowicznote\"></a>3.[^](#sobkowicz): Sobkowicz, Antoni & Stokowiec, Wojciech. (2016). Steam Review Dataset - new, large scale sentiment dataset. https://www.researchgate.net/publication/311677831_Steam_Review_Dataset_-_new_large_scale_sentiment_dataset<br> \n",
    "<a name=\"statistanote\"></a>4.[^](#statista): Statista.com. (2021). Number of peak concurrent Steam users from January 2013 to September 2021. https://www.statista.com/statistics/308330/number-stream-users/<br>\n",
    "<a name=\"zuonote\"></a>5.[^](#zuo): Zuo, Zhen. (2018). Sentiment Analysis of Steam Review Datasets using Naive Bayes and Decision Tree Classifier. IDEALS, 2018-07-03. http://hdl.handle.net/2142/100126<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
