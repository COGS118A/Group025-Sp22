{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A- Project Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Merel van den Bos\n",
    "- Alex Rivera\n",
    "- Albert Aung\n",
    "- Lillian Wood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents and how they are measured\n",
    "- what you will be doing with the data\n",
    "- how performance/success will be measured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Fill in the background and discuss the kind of prior work that has gone on in this research area here. **Use inline citation** to specify which references support which statements.  You can do that through HTML footnotes (demonstrated here). I used to reccommend Markdown footnotes (google is your friend) because they are simpler but recently I have had some problems with them working for me whereas HTML ones always work so far. So use the method that works for you, but do use inline citations.\n",
    "\n",
    "Here is an example of inline citation. After government genocide in the 20th century, real birds were replaced with surveillance drones designed to look just like birds<a name=\"lorenz\"></a>[<sup>[1]</sup>](#lorenznote). Use a minimum of 2 or 3 citations, but we prefer more <a name=\"admonish\"></a>[<sup>[2]</sup>](#admonishnote). You need enough citations to fully explain and back up important facts. \n",
    "\n",
    "Remeber you are trying to explain why someone would want to answer your question or why your hypothesis is in the form that you've stated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Clearly describe the problem that you are solving. Avoid ambiguous words. The problem described should be well defined and should have at least one ML-relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms), measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "- Dataset: https://www.kaggle.com/datasets/andrewmvd/steam-reviews\n",
    "- Description: The dataset contains over 6.4 million observations, which are publicly available reviews in English from the Steam Reviews portion of Steam store run by Valve. 5 variables describe each observation: Game id, Game Name, Review text, Review Sentiment: whether the game the review recommends the game or not, and Review vote: whether the review was recommended by another user or not.\n",
    "- Some critical variables are the Review text, Review sentiment, and Review vote. Review text will be string data. Review sentiment is coded -1 as negative and 1 as positive review. Review vote is coded 0 as not recommended and 1 as recommended.\n",
    "- Review sentiment and Review vote are already in numerical values, which alleviates cleaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "NEW SECTION!\n",
    "\n",
    "Please show any preliminary results you have managed to obtain.\n",
    "\n",
    "Examples would include:\n",
    "- Analyzing the suitability of a dataset or alogrithm for prediction/solving your problem \n",
    "- Performing feature selection or hand-designing features from the raw data. Describe the features available/created and/or show the code for selection/creation\n",
    "- Showing the performance of a base model/hyper-parameter setting.  Solve the task with one \"default\" algorithm and characterize the performance level of that base model.\n",
    "- Learning curves or validation curves for a particular model\n",
    "- Tables/graphs showing the performance of different models/hyper-parameters\n",
    "\n",
    "Our first step here was importing the dataset we are using. We will use the pandas library to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "df_steam = pd.read_csv(r'dataset.csv')\n",
    "df_steam = df_steam.head(100000) # Limit amount of data while testing\n",
    "df_steam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Cleaning on Review Text\n",
    "\n",
    "Next, we will need to clean the review text in order to process them through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_steam.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have some entries where either the app_name is null, or the review_text is null. As we do not care about the app_name, it is okay if some entries do not have this listed. However, we will remove all of the reviews where the review text is empty, as this is what we are anaylzing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_steam = df_steam.dropna(subset=['review_text'])\n",
    "df_steam.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to perform some preliminary cleaning on the review texts themselves to allow them to be processed by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    text_nonum = re.sub(r'\\d+', '', text)\n",
    "    text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in string.punctuation]) \n",
    "    text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text_nostop = \" \".join([word.lower() for word in text_no_doublespace.split() if word.lower() not in stopwords])\n",
    "    return text_nostop\n",
    "df_steam['review_text_clean'] = df_steam['review_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_steam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will attempt feature extraction using a bag of words approach. We will then run this bag of words through a TF-IDF transformation, and finally create a model using naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X = df_steam['review_text_clean']\n",
    "y = df_steam['review_score']\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_features=3000)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.20)\n",
    "\n",
    "classifier = clf.fit(X_train, y_train)\n",
    "\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we can see the confusion matrix from our initial run of the model using naive Bayes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cmat = confusion_matrix(y_test, predictions)\n",
    "\n",
    "sns.heatmap(cmat, annot=True, fmt='')\n",
    "\n",
    "print('Accuracy Rate: {}'.format(round(np.divide(np.sum([cmat[0,0],cmat[1,1]]),np.sum(cmat))), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our ethical review, we consulted the ‘’Data Science Ethics Checklist’’ by deon (https://deon.drivendata.org/), which contains check items for every data-related project. When people make a review on Steam, they get the option to choose whether it is public, only for their friends, or private.\n",
    "Nevertheless, we have chosen to use an anonymous data set for privacy reasons and avoid creating a potential bias by including personal information like gender for instance. Besides that, we also avoid the risk of unintended harm by choosing the anonymous data, where we avoid online abuse or physical harm being conducted upon the participants since their private information will be kept private in every way. \n",
    "Also, since some reviews are already private, this equalizes all the reviews together and Steam has provided this data. Their own website does have a privacy policy about for their users (https://store.steampowered.com/privacy_agreement/) so people must approve that. Despite the anonymity, we will also not use the data for any other purposes than our project. We have chosen a huge dataset which hopefully minimizes our potential bias. \n",
    "We are very aware of potential risks like p-hacking and wrongly sampling sizing and will pay attention to those. Any very odd outliers will be investigated and taken seriously. We know the source of our data and will share the dataset in our proposal for total transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We communicate through Discord where we either write each other and/or make calls, whatever is necessary for the discussion we need to have. We will also try to meet on campus in person in a while and work on the project together. \n",
    "We all respect each others way of working and we reach a consensus in good consultation. Also, we try to take everyones preferences considering topics for instance in consideration and so far this worked us very smoothly. When someone has a hard time makgin their deadline, we communicate this early upfront and help each other out.\n",
    "The tasks that have to be done are assigned to everyone based on their own preferences and competencies. Some might be assigned to everyone and some might be assigned to one person, as long as there is clear communication about it and everybody agrees this works for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATE THE PROPOSAL TIMELINE ACCORDING TO WHAT HAS ACTUALLY HAPPENED AND HOW IT HAS EFFECTED YOUR FUTURE PLANS\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 4/19  |  7 PM |  Brainstorm topics/questions | Discuss interesting topics; Determine best form of communication; Discuss approach to project | \n",
    "| 4/24  |  8 PM (talk over Discord) |  Edit the document with your piece of work | Help each other out with the final pieces; Discuss how to tackle our deadlines in the future; Submit first version of the project | \n",
    "| 5/7  | 10 AM  | Everyone had feedbacked another projct | Discuss how we are tackling our project; Assign group members to lead each specific part   |\n",
    "| 5/18  | 6 PM  | Talk to TA about feedback (Lily) | Discuss the TA's feedback; Discuss further handling/progress on assigned pieces   |\n",
    "| 5/20  | 10 PM  | Begin programming for project (Alex) | Discuss/edit project code; Submit Checkpoint |\n",
    "| 5/25  | 12 PM  | Complete analysis | Discuss/edit full project; Write discussion and conclusion of the project |\n",
    "| 6/8  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
